#!/bin/bash

#SBATCH --job-name=hpc          # Job name
#SBATCH --output=job_%j.out     # Output file (%j = SLURM job ID)
#SBATCH --error=job_%j.err      # Error file
#SBATCH --nodes=1               # Number of nodes (modifiy for MPI)
#SBATCH --ntasks-per-node=1     # Number of tasks per node
#SBATCH --cpus-per-task=1       # Number of threads per task (modify for multithreading)
#SBATCH --gres=gpu:A4000-Ada:1  # Number of GPUs per node
#SBATCH --time=00:15:00         # Time limit (HH:MM:SS)
#SBATCH --nice=100              # Job priority (allow higher priority jobs first)

PROG=cgc_serial
DATA=/var/scratch/bwn200/HPC_data/spring_data_m.npy
LABELS=/var/scratch/bwn200/HPC_data/spring_labels_m_3x20.txt
OUT=labels_${SLURM_JOB_ID}.txt

# Load required modules
module load openmpi4/4.1.1
module load cuda12.3/toolkit/12.3

# Set environment variables if needed (e.g., OMP_NUM_THREADS when using OpenMP)
#export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}

# Run the binary with arguments
mpirun ./${PROG} ${DATA} ${LABELS} -o ${OUT}
